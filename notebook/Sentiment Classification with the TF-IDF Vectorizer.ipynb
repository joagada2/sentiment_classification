{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882ed7cc",
   "metadata": {},
   "source": [
    "# SENTIMENT CLASSIFICATION WITH THE TF-IDF VECTORIZER\n",
    "Sentiment classification/analysis is one of the vital areas of Natural Language Processing (NLP). It is at the very centre of interaction of Machine Learning and NLP. Sentiment classification or sentiment analysis fall into broad category of text classification where a model is developed to take in text (document) and tell if the sentiment behind the text is positive, negetive or neutral. Sentiment Analysis is a supervised learning problem as it require a bunch of labelled texts as input. \n",
    "### Statement of Problem\n",
    "Given a document D and a set of fixed classes C = {c1, c2, ...,cn}, the problem of sentiment classification is that of determining the class c of D in C.\n",
    "### Definition of Important Terms\n",
    "In sentiment classification and NLP generally, the following terms are very important:\n",
    " - Document: Document in sentiment classification is group of words, phrase, sentence or a comlete article. A document could mean a tweet, part of news article, a whole news article, a product manual, a story etc. In tabular data, a document is the collection of words in text field of a particular record.\n",
    " - Corpus:  A corpus which a the list of all unique words in in all documents of a text dataset.\n",
    "### Method\n",
    "The Naive Bayes classification algorithm is favoured for this work as it is one of the most popular methods for sentiment classification. Naive Bayes method is easy and fast to use, suitable for multi-class classification and if its assumption of independence is satisfied, it perform better than other models and require much less training dataset.In this project, sentiment is classified as either negetive or positive, hence, the task is a binary problem. Consider a set of movie review, each with positive or negetive lable (that is, C = {c+, c-}), given a new review (document) d whose label (class) is not known, \n",
    " - p(c+/d) = p(d/c+)p(c+)/p(d) - (Bayes Rule)\n",
    "    - p(c+/d) is called the posterior probability\n",
    "    - p(d/c+) is called the likelihood\n",
    "    - p(c) is called the posterior probability\n",
    "    - p(d) is called the maximum likelihood\n",
    "A review d is assigned to class c+ if \n",
    " - p(d/c+)p(c+)/p(d)>p(d/c-)p(c-)/p(d)\n",
    "p(d) is the same across all classes, hence it can be dropped so the rule becomes to assign a review d to class c+ if\n",
    " - p(d/c+)p(c+)>p(d/c-)p(c-)\n",
    "p(c+) and P(c-) are the probabilities of positive and negetive reviews respectively in the training dataset> Hence,\n",
    " - p(c+) = r+/n where r+ = number of positive review and n is total number of review\n",
    " - P(c-) = r-/n where r- = number of negetive review and n -s total number of review\n",
    "To calculate p(d/c+) and p(d/c-), we vectorize the text dataset by either using the bag of words model or Term \n",
    "Frequency - Inverse Word Frequency (TF-IDF). Having vectorized the features and given that x1, x2, ...,xn are the numerical features\n",
    " - p(d/c+) = p(x1, x2, x3,...,xn/c+) and\n",
    " - p(d/c-) = p(x1, x2, x3,...,xn/c-)\n",
    "\n",
    "Assuming independence of x1, x2, ...,xn, the probabilities become\n",
    " - p(d/c+) = p(x1, x2, x3,...,xn/c+) = p(x1/c+)p(x2/c+)...p(xn/c+) -(Naive Bayes)\n",
    " - p(d/c-) = p(x1, x2, x3,...,xn/c-) = p(x1/c-)p(x2/c-)...p(xn/c-) - (Naive Bayes)\n",
    "When independence of the features are assumed, the Bayes rule becomes the Navive Bayes Rule as seen above. In this work, I will use the Multinomial Naive Bayes algorithm built into python sklearn for implementation.\n",
    "### Dataset\n",
    "In the project, a sentiment classification model will be applied to the Internet Movie Database (IMDB) Dataset of 50K Movie Reviews  downloaded from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe30e51e",
   "metadata": {},
   "source": [
    "## LIBRARY IMPORTATION AND LOADING OF DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f31667b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library importation\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c044557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "df = pd.read_csv('../data/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f8f12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3efbe8",
   "metadata": {},
   "source": [
    "## DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7192c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of unwanted characters and spaces\n",
    "def clean_data(data):\n",
    "    data = str(data).lower() #lower case\n",
    "    data = re.sub(r\"@\\S+ \", r' ',data) #remove all mentions and replace with a single empty space\n",
    "    data = re.sub('https://.*','',data) #remove all urls\n",
    "    data = re.sub(\"\\s+\",' ',data) #remove multiple spaces or tabs and replace with a single space\n",
    "    data = re.sub(\"\\n+\",' ',data) #remove multiple empty lines\n",
    "    letters = re.sub(\"[^a-zA-Z]\",' ',data) #take ontly text and ignore other non text characters\n",
    "    return letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87867646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply clean_data function\n",
    "df['review'] = df['review'].apply(lambda x:clean_data(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ddd7d3",
   "metadata": {},
   "source": [
    "##### Note that stopword removal, tokenization, stemming and conversion of text to lower cases are all performed by the TfidfVectorizer. For anyone using any other vectorizer, the cleaning process can be continued with the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bed32d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset = set(stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(use_idf = True, lowercase = True, strip_accents = 'ascii', stop_words = stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb78ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of dependent variable\n",
    "y = df.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c6462d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of independent variable\n",
    "x = vectorizer.fit_transform(df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7dfb9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    positive\n",
       "1    positive\n",
       "2    positive\n",
       "3    negative\n",
       "4    positive\n",
       "Name: sentiment, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "87598810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f0615a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 99248)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7282edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into training and testing dataset\n",
    "x_train,x_test, y_train,y_test = train_test_split(x,y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59150b",
   "metadata": {},
   "source": [
    "## MODEL CREATION AND TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32023046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the Multinomial Naive Mayes Model\n",
    "model = naive_bayes.MultinomialNB()\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3be99e",
   "metadata": {},
   "source": [
    "## MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a8c6ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9408122901887448"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check acuracy of model\n",
    "roc_auc_score(y_test,model.predict_proba(x_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8399578",
   "metadata": {},
   "source": [
    "## APPLICATION OF MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77c3f6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive']\n"
     ]
    }
   ],
   "source": [
    "#apply model to new review\n",
    "print(model.predict(vectorizer.transform(np.array([\"The movie is a nice movie and I enjoyed watchin\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "669609f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative']\n"
     ]
    }
   ],
   "source": [
    "pred = print(model.predict(vectorizer.transform(np.array([\"The movie is a bad movie but I wasted time watching\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae64620",
   "metadata": {},
   "source": [
    "## SAVING AND LOADING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c0e573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "986196ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agada_sentiment_classifier']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'agada_sentiment_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0373261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agada_sentiment_vectorizer']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vectorizer, 'agada_sentiment_vectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e41e53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('agada_sentiment_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bdc7de95",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = joblib.load('agada_sentiment_vectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "713e2f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(vectorizer.transform(np.array(['\"The movie is a bad movie but I wasted time watching\"'])))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6b01a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'Good movie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aeddce75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(vectorizer.transform(np.array([text1])))[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
